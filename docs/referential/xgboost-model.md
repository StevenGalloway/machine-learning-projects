# XGBoost (Extreme Gradient Boosting)

XGBoost is a **high-performance ensemble machine learning algorithm**
based on **gradient boosting over decision trees**.

It is one of the most widely used algorithms in:
- Kaggle competitions
- Industry ML systems
- Tabular data problems

XGBoost stands for **Extreme Gradient Boosting**.

------------------------------------------------------------------------

## Core Intuition

XGBoost answers the question:

> *“How can I combine many weak decision trees into a single powerful model?”*

It does this by:
- Training trees **sequentially**
- Each new tree focuses on **correcting the mistakes** of previous trees
- Optimizing a global **loss function** using gradient descent

------------------------------------------------------------------------

## Why It’s Called “Extreme”

XGBoost is called *“Extreme”* because it adds:

- Highly optimized C++ implementation
- Parallel processing
- Regularization
- Tree pruning
- Advanced loss functions
- GPU acceleration
- Memory efficiency

It is essentially **production-grade gradient boosting**.

------------------------------------------------------------------------

## Key Concepts & Keywords

### Gradient Boosting

A technique where:
- Models are trained **one after another**
- Each model learns from the **residual errors** of the previous model

\[
FinalModel = Tree_1 + Tree_2 + Tree_3 + ...
\]

------------------------------------------------------------------------

### Weak Learners

Each individual tree is:
- Shallow
- Simple
- Slightly better than random guessing

Power comes from **combining many weak models**.

------------------------------------------------------------------------

### Loss Function

Defines what “error” means. The gap between a prediction and the true label into a penalty number.

We want to lower the penalty number as much as possible.

Examples:
- Mean Squared Error (regression)
- Log Loss (classification) (most popular) - Heavily punishes wrong predictions while rewarding correct predictions. Future trees learn from corrections made by previous trees.
- Custom business loss functions

------------------------------------------------------------------------

### Gradient Descent

XGBoost uses **gradients of the loss function**
to determine how the next tree should correct errors.

Hence:
> **Gradient Boosting**

------------------------------------------------------------------------

### Regularization

XGBoost penalizes model complexity using:

- L1 (Lasso)
- L2 (Ridge)
- Tree depth constraints
- Minimum child weight

This reduces **overfitting**.

------------------------------------------------------------------------

## How XGBoost Works (Step-by-Step)

1. Train first decision tree
2. Compute prediction errors (residuals)
3. Train next tree on residuals
4. Repeat for N trees
5. Sum all trees’ outputs
6. Apply regularization
7. Output final prediction

------------------------------------------------------------------------

## Relationship to Other Models

| Model | Relationship |
|------|-------------|
| Decision Trees | Base building blocks |
| Random Forest | Bagging instead of boosting |
| AdaBoost | Early boosting algorithm |
| GBM | Generic gradient boosting |
| LightGBM | Faster variant |
| CatBoost | Handles categorical features |
| Neural Networks | Alternative nonlinear models |

------------------------------------------------------------------------

## Why XGBoost Is So Popular

Because it works extremely well for:

- Tabular structured data
- Business ML problems
- Small-to-medium datasets
- Problems with noisy features

In many real-world problems, XGBoost **outperforms deep learning**.

------------------------------------------------------------------------

## Use Cases

### Finance
- Credit risk scoring
- Fraud detection
- Default prediction

Example: [Loan Approval](case-studies\loan-approval-xgb\scripts\train_eval.py)

### Marketing
- Customer churn
- Lead scoring
- Campaign optimization

Example: (Insert Example Repo when completed)

### Operations
- Demand forecasting
- Failure prediction
- Anomaly detection

Example: (Insert Example Repo when completed)

### Data Science Competitions
- Kaggle tabular challenges
- Feature engineering benchmarks

Example: (Insert Example Repo when completed)

------------------------------------------------------------------------

## Pros

- State-of-the-art performance on tabular data
- Built-in regularization
- Handles missing values
- Interpretable via feature importance
- Extremely fast
- Scales to large datasets
- Supports custom loss functions

------------------------------------------------------------------------

## Cons

- Many hyperparameters
- Can overfit if misconfigured
- Less interpretable than linear models
- Not ideal for images/audio/text embeddings
- Training can be expensive at scale

------------------------------------------------------------------------

## When XGBoost Excels

- Structured business data
- Feature-engineered datasets
- No strong temporal dependencies
- Medium-sized datasets
- High signal-to-noise ratio

------------------------------------------------------------------------

## When XGBoost Performs Poorly

- Raw image data
- Audio processing
- Natural language embeddings
- Long time series
- Reinforcement learning
